Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{FIoA1997,
abstract = {Section 2B of the Supplementary Introduction to Volume 1 gives a general description of reserving methodology. In that description, the process of arriving at an estimate of future payments is described as one of constructing a model, fitting it to some set of past observations, and using it to infer results about the future-in this case, the future events we are interested in are the payment of claims. Several distinctions are made between different types of model, including those between deterministic and stochastic models. Deterministic reserving models are, broadly, those which only make assumptions about the expected value of future payments. Stochastic models also model the variation of those future payments. By making assumptions about the random component of a model, stochastic models allow the validity of the assumptions to be tested statistically, and produce estimates not only of the expected value of the future payments, but also of the variation about that expected value. All the methods in Volume 2 could be described as stochastic to a greater or lesser extent. One can distinguish between them a little, since the methods described in Sections D1, D4, D5, D6 and D7 all allow the user to make estimates of the variation about the expected future payments. The methods described in sections D2 and D3, however, simply involve the fitting of curves to sets of data. The curves are then used to predict future payments, but do not allow the modeller to make estimates of the variation of these payments. A further distinction can be made between those models based on individual claims, and those which project grouped claims data. This distinction is most commonly found amongst stochastic methods, although the only methods presently in Volume 2 which model individual claims information are those explained in Sections D4 and D7.},
author = {{Faculty and Institute of Actuaries}},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Faculty and Institute of Actuaries - 1997 - Description of stochastic models, what is a stochastic model.pdf:pdf},
journal = {Claims Reserving Manual},
month = {sep},
pages = {B1.1--B3.2},
title = {{Description of stochastic models, what is a stochastic model?}},
volume = {v2},
year = {1997}
}
@article{Li2009,
abstract = {Traditionally, actuaries have modeled mortality improvement using deterministic reduction factors, with little consideration of the associated uncertainty. As mortality improvement has become an increasingly significant source of financial risk, it has become important to measure the uncertainty in the forecasts. Probabilistic confidence intervals provided by the widely accepted Lee-Carter model are known to be excessively narrow, due primarily to the rigid structure of the model. In this paper, we relax the model structure by considering individual differences (heterogeneity) in each age-period cell. The proposed extension not only provides a better goodness-of-fit based on standard model selection criteria, but also ensures more conservative interval forecasts of central death rates and hence can better reflect the uncertainty entailed. We illustrate the results using US and Canadian mortality data.},
author = {Li, Johnny Siu-Hang and Hardy, Mary R. and Tan, Ken Seng},
doi = {10.2143/ast.39.1.2038060},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Li, Hardy, Tan - 2009 - Uncertainty in Mortality Forecasting An Extension to the Classical Lee-Carter Approach.pdf:pdf},
issn = {0515-0361},
journal = {ASTIN Bulletin},
number = {1},
pages = {137--164},
title = {{Uncertainty in Mortality Forecasting: An Extension to the Classical Lee-Carter Approach}},
volume = {39},
year = {2009}
}
@article{Kalotay1993,
author = {Kalotay, Andrew J and Williams, George O and Fabozzi, Frank J},
doi = {10.2469/faj.v49.n3.35},
issn = {0015-198X},
journal = {Financial Analysts Journal},
number = {3},
pages = {35--46},
title = {{A Model for Valuing Bonds and Embedded Options}},
volume = {49},
year = {1993}
}
@misc{FIoA1993,
author = {{Faculty and Institute of Actuaries}},
booktitle = { General Insurance Convention},
title = {{Prudential Margins}},
url = {https://www.actuaries.org.uk/system/files/documents/pdf/0127-0149_0.pdf},
urldate = {2021-09-01},
year = {1993}
}
@misc{Kagan2021,
author = {Kagan, Julia},
month = {jan},
title = {{Valuation Reserve Definition}},
url = {https://www.investopedia.com/terms/v/valuation-reserve.asp},
urldate = {2021-04-09},
year = {2021}
}
@misc{HMD2021,
author = {{The Human Mortality Database}},
title = {{HMD U.K., England and Wales Total Population}},
url = {https://www.mortality.org/},
urldate = {2021-08-23},
year = {2021}
}
@article{Cairns2006b,
author = {Cairns, Andrew J G and Dowd, Kevin},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Cairns, Dowd - 2006 - A Two-Factor Model for Stochastic Mortality With Parameter Uncertainty.pdf:pdf},
journal = {Journal of Risk and Insurance},
number = {4},
pages = {687--718},
title = {{A Two-Factor Model for Stochastic Mortality With Parameter Uncertainty:}},
volume = {73},
year = {2006}
}
@misc{Singh2018,
author = {Singh, Seema},
title = {{Understanding the Bias-Variance Tradeoff | by Seema Singh | Towards Data Science}},
url = {https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229},
urldate = {2021-08-28},
year = {2018}
}
@article{Alho1990,
abstract = {This paper presents a stochastic version of the demographic cohort-component method of forecasting future population. In this model the sizes of future age-sex groups are non-linear functions of random future vital rates. An approximation to their joint distribution can be obtained using linear approximations or simulation. A stochastic formulation points to the need for new empirical work on both the autocorrelations and the cross-correlations of the vital rates. Problems of forecasting declining mortality and fluctuating fertility are contrasted. A volatility measure for fertility is presented. The model can be used to calculate approximate prediction intervals for births using data from deterministic cohort-component forecasts. The paper compares the use of expert opinion in mortality forecasting with simple extrapolation techniques to see how useful each approach has been in the past. Data from the United States suggest that expert opinion may have caused systematic bias in the forecasts. {\textcopyright} 1991.},
author = {Alho, Juha M.},
doi = {10.1016/0169-2070(90)90030-F},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Alho - 1990 - Stochastic methods in population forecasting.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Cohort-component,Expert opinion,Fertility,Mortality,Volatility},
number = {4},
pages = {521--530},
pmid = {12285033},
title = {{Stochastic methods in population forecasting}},
volume = {6},
year = {1990}
}
@article{Richards2004,
author = {Richards, Stephen},
doi = {10.1002/9780470012505.tap033},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Richards - 2004 - Profit Testing.pdf:pdf},
isbn = {9780470012505},
journal = {Encyclopedia of Actuarial Science},
number = {September 2006},
title = {{Profit Testing}},
year = {2004}
}
@article{Lee1992,
abstract = {Time series methods are used to make long-run forecasts, with confidence intervals, of age-specific mortality in the United States from 1990 to 2065. First, the logs of the age-specific death rates are modeled as a linear function of an unobserved period-specific intensity index, with parameters depending on age. This model is fit to the matrix of U.S. death rates, 1933 to 1987, using the singular value decomposition (SVD) method; it accounts for almost all the variance over time in age-specific death rates as a group. Whereas eo has risen at a decreasing rate over the century and has decreasing variability, k(t) declines at a roughly constant rate and has roughly constant variability, facilitating forecasting. k(t), which indexes the intensity of mortality, is next modeled as a time series (specifically, a random walk with drift) and forecast. The method performs very well on within-sample forecasts, and the forecasts are insensitive to reductions in the length of the base period from 90 to 30 years; some instability appears for base periods of 10 or 20 years, however. Forecasts of age-specific rates are derived from the forecasts of k, and other life table variables are derived and presented. These imply an increase of 10.5 years in life expectancy to 86.05 in 2065 (sexes combined), with a confidence band of plus 3.9 or minus 5.6 years, including uncertainty concerning the estimated trend. Whereas 46% now survive to age 80, by 2065 46% will survive to age 90. Of the gains forecast for person-years lived over the life cycle from now until 2065, 74% will occur at age 65 and over. These life expectancy forecasts are substantially lower than direct time series forecasts of eo, and have far narrower confidence bands; however, they are substantially higher than the forecasts of the Social Security Administration's Office of the Actuary.},
author = {Lee, Ronald D. and Carter, Lawrence R.},
doi = {10.2307/2290201},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop//Lee, Carter - 1992 - Modeling and Forecasting U. S. Mortality.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {75 years,continue to rise at,data and evaluate,demographic model to u,demography,forecast,from 1900 to 1988,if it were to,life expectancy,life expectancy in the,mortality,next we fit the,population,projection,rose from 47 to,s,this,united states},
number = {419},
pages = {659},
title = {{Modeling and Forecasting U. S. Mortality}},
volume = {87},
year = {1992}
}
@article{Smart1977,
abstract = {1.1. Profits are essential to the economic viability and indeed the survival of any enterprise. Life assurance companies are no exception whether they be mutual or proprietary, Unless profit—or as it is often termed, surplus—is available for distribution to shareholders and/or policyholders, the reasonable expectations of shareholders or policyholders will not be met: nor will the office be able to accumulate and maintain adequate contingency funds.1.2. The word profit will be used synonymously with surplus throughout this paper. Profit is defined as the excess during any period of income over outgo, where outgo includes the necessary increase in valuation reserves.},
author = {Smart, I. C.},
doi = {10.1017/s0020268100018187},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Smart - 1977 - Pricing and profitability in a life office.pdf:pdf},
issn = {0020-2681},
journal = {Journal of the Institute of Actuaries},
number = {2},
pages = {125--172},
title = {{Pricing and profitability in a life office}},
volume = {104},
year = {1977}
}
@article{CHAN1992,
abstract = {We estimate and compare a variety of continuous‐time models of the short‐term riskless rate using the Generalized Method of Moments. We find that the most successful models in capturing the dynamics of the short‐term interest rate are those that allow the volatility of interest rate changes to be highly sensitive to the level of the riskless rate. A number of well‐known models perform poorly in the comparisons because of their implicit restrictions on term structure volatility. We show that these results have important implications for the use of different term structure models in valuing interest rate contingent claims and in hedging interest rate risk. 1992 The American Finance Association},
author = {CHAN, K C and KAROLYI, G ANDREW and LONGSTAFF, FRANCIS A and SANDERS, ANTHONY B},
doi = {10.1111/j.1540-6261.1992.tb04011.x},
issn = {15406261},
journal = {The Journal of Finance},
number = {3},
pages = {1209--1227},
title = {{An Empirical Comparison of Alternative Models of the Short‐Term Interest Rate}},
volume = {47},
year = {1992}
}
@article{Koissi2006,
abstract = {This paper first studies the performance of the Lee-Carter [J. Am. Stat. Assoc. 419 (87) (1992) 659-675] model for mortality forecasting on the Nordic countries. Three approaches for computing the model parameters are compared: Singular Value Decomposition, Weighted Least Square and Maximum Likelihood Estimation. Hypothetical projections are also made, based on variable period intervals. Secondly, the paper addresses an extension to the Lee-Carter method: A residual bootstrapped technique is used to construct confidence intervals for forecasted life expectancies. Uncertainties produced with this method incorporate the variability from all parameters in the model, while the original Lee-Carter method focuses on the variability in the time-varying parameter. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Koissi, Marie Claire and Shapiro, Arnold F. and H{\"{o}}gn{\"{a}}s, G{\"{o}}ran},
doi = {10.1016/j.insmatheco.2005.06.008},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Koissi, Shapiro, H{\"{o}}gn{\"{a}}s - 2006 - Evaluating and extending the Lee-Carter model for mortality forecasting Bootstrap confidence interval.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Bootstrap methods,Confidence interval,Lee-Carter method,Mortality forecasting,Stochastic model},
number = {1},
pages = {1--20},
title = {{Evaluating and extending the Lee-Carter model for mortality forecasting: Bootstrap confidence interval}},
volume = {38},
year = {2006}
}
@article{BenSalah2014,
abstract = {This article attempts to identify the best model of the short term interest rates that can predict its stochastic process over time. We studied nine different models of the short term interest rates. The choice of these models was the aim of analyzing the relevance of certain specifications of the the short term interest rate stochastic process, the effect of mean reversion and the sensitivity of the volatility to the level of interest rate.  The yield on US three months treasury bills is used as a proxy for the short term interest rates. The parameters of the different stochastic process are estimated using the generalized method of moments. The results show that the effect of mean reversion is not statistically significant and that volatility is highly sensitive to the level of interest rates. To further study the performance prediction of the intertemporal behavior of the short term interest rate of the various models; we simulated their stochastic process for different periods.  The results show that none of the studied models reproduce the actual path of the short term interest rates. The problem lies in the parametric specification of the mean and volatility of the diffusion process To further study the accurate parametric specification of the interest rate stochastic process we use a nonparametric estimation of the drift and the diffusion functions. The results prove that both should be nonlinear.},
author = {{Ben Salah}, Mona},
doi = {10.2139/ssrn.2393909},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {diffusion process,gmm,likelihood,maximum,monte carlo simulation,nonlinear,nonparametric,short term interest rate},
number = {2},
pages = {41--60},
title = {{Comparison of the Short Term Interest Rate Models: Parametric Versus Non Parametric Approach}},
volume = {4},
year = {2014}
}
@article{Cairns2011a,
abstract = {This paper develops a framework for developing forecasts of future mortality rates. We discuss the suitability of six stochastic mortality models for forecasting future mortality and estimating the density of mortality rates at different ages. In particular, the models are assessed individually with reference to the following qualitative criteria that focus on the plausibility of their forecasts: biological reasonableness; the plausibility of predicted levels of uncertainty in forecasts at different ages; and the robustness of the forecasts relative to the sample period used to fit the model. An important, though unsurprising, conclusion is that a good fit to historical data does not guarantee sensible forecasts. We also discuss the issue of model risk, common to many modelling situations in demography and elsewhere. We find that even for those models satisfying our qualitative criteria, there are significant differences among central forecasts of mortality rates at different ages and among the distributions surrounding those central forecasts. {\textcopyright} 2011 Elsevier B.V.},
author = {Cairns, Andrew J.G. and Blake, David and Dowd, Kevin and Coughlan, Guy D. and Epstein, David and Khalaf-Allah, Marwa},
doi = {10.1016/j.insmatheco.2010.12.005},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Cairns et al. - 2011 - Mortality density forecasts An analysis of six stochastic mortality models.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Fan charts,Forecasting,Model risk,Model selection criteria,Plausibility},
number = {3},
pages = {355--367},
publisher = {Elsevier B.V.},
title = {{Mortality density forecasts: An analysis of six stochastic mortality models}},
url = {http://dx.doi.org/10.1016/j.insmatheco.2010.12.005},
volume = {48},
year = {2011}
}
@article{Brigo2007,
author = {Brigo, Damiano},
journal = {Theory and Practice},
title = {{Interest Rate Models : Paradigm shifts in recent years}},
volume = {2001},
year = {2007}
}
@misc{Lioudis2021,
author = {Lioudis, Nick},
booktitle = {What is the Gold Standard?},
month = {may},
title = {{What is the Gold Standard?}},
url = {https://www.investopedia.com/ask/answers/09/gold-standard.asp},
urldate = {2021-05-16},
year = {2021}
}
@article{Brouhns2002,
abstract = {This paper implements Wilmoth's [Computational methods for fitting and extrapolating the Lee-Carter model of mortality change, Technical report, Department of Demography, University of California, Berkeley] and Alho's [North American Actuarial Journal 4 (2000) 91] recommendation for improving the Lee-Carter approach to the forecasting of demographic components. Specifically, the original method is embedded in a Poisson regression model, which is perfectly suited for age-sex-specific mortality rates. This model is fitted for each sex to a set of age-specific Belgian death rates. A time-varying index of mortality is forecasted in an ARIMA framework. These forecasts are used to generate projected age-specific mortality rates, life expectancies and life annuities net single premiums. Finally, a Brass-type relational model is proposed to adapt the projections to the annuitants population, allowing for estimating the cost of adverse selection in the Belgian whole life annuity market. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Brouhns, Natacha and Denuit, Michael and Vermunt, Joroen K.},
doi = {10.1016/S0167-6687(02)00185-3},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Brouhns, Denuit, Vermunt - 2002 - A Poisson log-bilinear regression approach to the construction of projected lifetables.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Adverse selection,Age-sex-specific mortality,Annuities,Life insurance,Lifetable functions},
number = {3},
pages = {373--393},
title = {{A Poisson log-bilinear regression approach to the construction of projected lifetables}},
volume = {31},
year = {2002}
}
@article{Cox-ingersoll-ross,
author = {Cox-ingersoll-ross, The},
journal = {Econometrica},
number = {2},
pages = {385--407},
title = {{Cox-Ingersoll-Ross model}},
volume = {53},
year = {1985}
}
@misc{Gaebler2014,
author = {Gaebler, Ralph F.},
booktitle = {Sources of State Practice in International Law: Second Revised Edition},
doi = {10.1163/9789004272224_029},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Gaebler - 2014 - Republic of South Africa Insurance Bill.pdf:pdf},
isbn = {9789004272224},
number = {39403},
pages = {468--481},
title = {{Republic of South Africa Insurance Bill}},
year = {2014}
}
@article{Dothan,
abstract = {The paper presents a valuation formula for default free bonds for a certain class of tastes when the instantaneously riskfree rate of interest follows a geometric Wiener process. Properties of the resulting term structure of interest rates are studied, and an application of the analysis to the pricing of Treasury Bills is proposed. {\textcopyright} 1978.},
author = {Dothan, L. Uri},
doi = {10.1016/0304-405X(78)90020-X},
issn = {0304-405X},
journal = {Journal of Financial Economics},
month = {mar},
number = {1},
pages = {59--69},
publisher = {North-Holland},
title = {{On the term structure of interest rates}},
volume = {6},
year = {1978}
}
@article{Burgess2018,
abstract = {The Vasicek model (1977) is one of the earliest stochastic models of the term structure of interest rates. This model, though it has it's shortcomings, has many advantages, such as analytical tractability and mean reversion features, and may be viewed as a short rate model template.  Several short rate models have their foundations rooted in the Vasicek model. The classical Hull-White model (1990a), for example, is an extension of the Vasicek model with time dependent parameters.  In the work that follows we derive the short rate implied by the Vasicek model using the integrating factor method and provide an overview of this method and it's shorthand. Secondly we consider the model dynamics and finally we apply the model to zero-coupon bond pricing and provide a detailed derivation.   Finally in reviewing the Vasicek model we outline it's disadvantages, consider other short rate models and look at the Hull-White extension to this model. The aim of the paper is to provide an overview of the Vasicek model and an introduction into short rate modelling.},
author = {Burgess, Nicholas},
doi = {10.2139/ssrn.2479671},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {bond pricing,short rate models,vasicek model},
number = {November 2013},
pages = {1--16},
title = {{An Overview of the Vasicek Short Rate Model}},
year = {2018}
}
@article{Cairns2006,
abstract = {In this article, we consider the evolution of the post-age-60 mortality curve in the United Kingdom and its impact on the pricing of the risk associated with aggregate mortality improvements over time: so-called longevity risk. We introduce a two-factor stochastic model for the development of this curve through time. The first factor affects mortality-rate dynamics at all ages in the same way, whereas the second factor affects mortality-rate dynamics at higher ages much more than at lower ages. The article then examines the pricing of longevity bonds with different terms to maturity referenced to different cohorts. We find that longevity risk over relatively short time horizons is very low, but at horizons in excess of ten years it begins to pick up very rapidly. A key component of the article is the proposal and development of a method for calculating the market risk-adjusted price of a longevity bond. The proposed adjustment includes not just an allowance for the underlying stochastic mortality, but also makes an allowance for parameter risk. We utilize the pricing information contained in the November 2004 European Investment Bank longevity bond to make inferences about the likely market prices of the risks in the model. Based on these, we investigate how future issues might be priced to ensure an absence of arbitrage between bonds with different characteristics. {\textcopyright} The Journal of Risk and Insurance, 2006,.},
author = {Cairns, Andrew J.G. and Blake, David and Dowd, Kevin},
doi = {10.1111/j.1539-6975.2006.00195.x},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Cairns, Blake, Dowd - 2006 - A two-factor model for stochastic mortality with parameter uncertainty Theory and calibration.pdf:pdf},
issn = {00224367},
journal = {Journal of Risk and Insurance},
number = {4},
pages = {687--718},
title = {{A two-factor model for stochastic mortality with parameter uncertainty: Theory and calibration}},
volume = {73},
year = {2006}
}
@article{Olivieri2001,
author = {Olivieri, A},
journal = {Insurance: Mathematics and Economics},
number = {27},
pages = {231--245},
title = {{Uncertainty in mortality projections: an actuarial perspective}},
year = {2001}
}
@article{Li2015,
abstract = {Two-population stochastic mortality models play a crucial role in the securitization of longevity risk. In particular, they allow us to quantify the population basis risk when longevity hedges are built from broad-based mortality indexes. In this paper, we propose and illustrate a systematic process for constructing a two-population mortality model for a pair of populations. The process encompasses four steps, namely (1) determining the conditions for biological reasonableness, (2) identifying an appropriate base model specification, (3) choosing a suitable time-series process and correlation structure for projecting period and/or cohort effects into the future, and (4) model evaluation.For each of the seven single-population models from Cairns et al. (2009), we propose two-population generalizations. We derive criteria required to avoid long-term divergence problems and the likelihood functions for estimating the models. We also explain how the parameter estimates are found, and how the models are systematically simplified to optimize the fit based on the Bayes Information Criterion. Throughout the paper, the results and methodology are illustrated using real data from two pairs of populations.},
author = {Li, Johnny Siu Hang and Zhou, Rui and Hardy, Mary},
doi = {10.1016/j.insmatheco.2015.03.021},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Li, Zhou, Hardy - 2015 - A step-by-step guide to building two-population stochastic mortality models.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Coherent mortality forecasting,Evaluation of mortality models,Index-based longevity hedges,Population basis risk,Selection of mortality models},
pages = {121--134},
publisher = {Elsevier B.V.},
title = {{A step-by-step guide to building two-population stochastic mortality models}},
url = {http://dx.doi.org/10.1016/j.insmatheco.2015.03.021},
volume = {63},
year = {2015}
}
@misc{Chen2021,
author = {Chen, James},
month = {jan},
title = {{Reserve Fund Definition}},
url = {https://www.investopedia.com/terms/r/reservefund.asp},
urldate = {2021-03-26},
year = {2021}
}
@article{Sanders2017,
abstract = {As machine learning has become more common within computer security, researchers have focused on improving machine learning architectures to better detect attacks. But what if the data we're using to train and evaluate our models is sub-optimal? Cybersecurity machine learning researchers face two main problems when acquiring data to train models. First, any available data is necessarily old and potentially outdated in comparison to the environment the model will face on deployment. Second , researchers may not even have access to relevant past data, often due to privacy concerns. The greatest model, trained on data inconsistent with the data it actually faces in the real world, will at best perform unreliably , and at worst fail catastrophically. In short, the worse the data that goes into a machine learning system, the worse the results that come out. How do we mitigate this problem in a security context? In this paper, we describe one remediation that we've found helps us estimate the impact of, and mitigate this issue. Specifically , we present sensitivity results from the same deep learning model designed to detect malicious URLs, trained and tested across 3 different sources of URL data. We review the methodology , results, and learnings from this analysis. NOMENCLATURE AUC Area under the curve, where the curve is a ROC curve (Receiver Operating Characteristic). A value of .5 is bad-it represents the predictive capability of a coin flip, whereas 1 represents perfectly ordered predictions. train set Data used to train a machine learning model. Independent (separate) from the test set data. test set Data used to test a machine learning model. Independent (separate) from the training data. Often, test data is referred to as 'holdout' data, because it is withheld from training so as to be used for testing. INTRODUCTION As the volume and sophistication of new cyber threats has grown, signature based attack detection methods have struggled to keep up. The security community has adapted by increasingly pursuing machine learning based detection approaches. Often, papers and discussions focus only on the machine learning systems themselves, with little mention of identifying the right data with which to train and test these systems. But in a cybersecurity context, where attackers and networks evolve over time and data differs from network to network, the training and test data we use to create and evaluate our systems matters greatly. How do we pick the best data with which to train our systems , and how do we use our data to predict how well our systems will detect attacks once we deploy them in a new data environ-ment? In cybersecurity machine learning, we never have perfect data with which to train and test our machine learning models, due to several factors:},
author = {Sanders, Hillary and Saxe, Joshua},
file = {::},
title = {{Garbage in, garbage out: how purportedly great ml models can be screwed up by bad data}},
url = {https://www.blackhat.com/docs/us-17/wednesday/us-17-Sanders-Garbage-In-Garbage-Out-How-Purportedly-Great-ML-Models-Can-Be-Screwed-Up-By-Bad-Data-wp.pdf},
year = {2017}
}
@article{Booth2008,
abstract = {Continuing increases in life expectancy beyond previously-held limits have brought to the fore the critical importance of mortality forecasting. Significant developments in mortality forecasting since 1980 are reviewed under three broad approaches: expectation, extrapolation and explanation. Expectation is not generally a good basis for mortality forecasting, as it is subjective; expert expectations are invariably conservative. Explanation is restricted to certain causes of death with known determinants. Decomposition by cause of death poses problems associated with the lack of independence among causes and data difficulties. Most developments have been in extrapolative forecasting, and make use of statistical methods rather than models developed primarily for age-specific graduation. Methods using two-factor models (age-period or age-cohort) have been most successful. The two-factor Lee^Carter method, and, in particular, its variants, have been successful in terms of accuracy, while recent advances have improved the estimation of forecast uncertainty. Regression-based (GLM) methods have been less successful, due to nonlinearities in time. Three-factor methods are more recent; the Lee^Carter age-period- cohort model appears promising. Specialised software has been developed and made available. Research needs include further comparative evaluations of methods in terms of the accuracy of the point forecast and its uncertainty, encompassing a wide range of mortality situations. keywords},
annote = {Sources of forcasting errors Alho (1990):
* model misspecification
* parameter estimation
* errors in informed judgement
* inherent random variation

"For long-term forecasting, in particular, the choice between models cannot reliably be based on historical goodness of fit. Ideally, out-of-sample or ex-post forecast errors should be examined using historical data."

"Two related considerations are the choice of measure used in the measurement of forecast accuracy and the specification of an appropriate loss function (Ahlburg, 1995; Booth et al., 2006; Pedroza,
2006)."},
author = {Booth, H. and Tickle, L.},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Booth, Tickle - 2008 - Mortality modelling and forecasting a review of methods By H. Booth and L. Tickle.pdf:pdf},
journal = {Annals of actuarial science},
keywords = {Cause of Death,Cohort,Decomposition,Extrapolation,Forecasting,GLM,Lee-Carter,Modelling,Mortality,Software,Uncertainty,contact,p-splines},
number = {I/II},
pages = {3--43},
title = {{Mortality modelling and forecasting: a review of methods By H. Booth and L. Tickle}},
volume = {3},
year = {2008}
}
@article{Urb2007,
author = {Urb, Alexandra},
title = {{Calibration of term structure models Dissertation Thesis}},
year = {2007}
}
@misc{Tse1995,
author = {Tse, Y K},
booktitle = {International Congress on Modelling and Simulation Proceedings},
pages = {13--34},
title = {{Stochastic models of interest rates in economics, finance and actuarial science}},
volume = {4},
year = {1995}
}
@misc{Robins2020,
author = {Robins, Adam},
title = {{Stochastic vs Deterministic Models: Understand the Pros and Cons}},
url = {https://blog.ev.uk/stochastic-vs-deterministic-models-understand-the-pros-and-cons},
urldate = {2021-08-28},
year = {2020}
}
@unpublished{Hunt2015,
abstract = {There has recently been a huge increase in the use of models which examine the structure of mortality rates across the dimensions of age, period and cohort. This paper reviews the major develop-ments in the field and provides a holistic analysis of these models and examines their similarities and differences. Specifically, it reviews the models that have been proposed to date, investigates the struc-ture of age/period/cohort mortality models, introduces a classification scheme for existing models and lists the key principles a model user should consider when constructing a new model in this class.},
author = {Hunt, Andrew and Blake, David},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Hunt, Blake - 2015 - On the Structure and Classification of Mortality Models. Pension Institute. Working Paper.pdf:pdf},
number = {August},
title = {{On the Structure and Classification of Mortality Models. Pension Institute. Working Paper.}},
year = {2015}
}
@article{Azman2020,
abstract = {The Lee–Carter model is a well-known model in modeling mortality. We aim to compare three probability models (Poisson, negative binomial and binomial) based on the Generalized Linear Model (GLM) framework of the Lee–Carter model. These models are applied to mortality data for 10 selected countries (Japan, United States, United Kingdom, Australia, Sweden, Spain, Belgium, Canada, Netherlands and Bulgaria) and the fit of these models is assessed using the deviance statistics and standardized residuals against fitted value plot. Among these three models, the negative binomial Lee–Carter model gave the best fit based on the deviance statistics and estimates of the log of deaths.},
author = {Azman, Shafiqah and Pathmanathan, Dharini},
doi = {10.1080/02664763.2020.1833183},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Azman, Pathmanathan - 2020 - The GLM framework of the Lee–Carter model a multi-country study.pdf:pdf},
issn = {13600532},
journal = {Journal of Applied Statistics},
keywords = {Demography,Lee–Carter model,generalized linear model,mortality,negative binomial},
title = {{The GLM framework of the Lee–Carter model: a multi-country study}},
url = {https://doi.org/10.1080/02664763.2020.1833183},
year = {2020}
}
@article{Piegorsch1990,
abstract = {A follow-up investigation to that given by Clark and Perry (1989, Biometrics 45, 309-316) is presented, giving details for maximum likelihood estimation for the dispersion parameter from a negative binomial distribution.},
author = {Piegorsch, Walter W.},
doi = {10.2307/2532104},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Piegorsch - 1990 - Maximum Likelihood Estimation for the Negative Binomial Dispersion Parameter.pdf:pdf},
issn = {0006341X},
journal = {Biometrics},
number = {3},
pages = {863},
pmid = {2242417},
title = {{Maximum Likelihood Estimation for the Negative Binomial Dispersion Parameter}},
volume = {46},
year = {1990}
}
@article{PaulJoshi1999,
abstract = {The advent of derivatives and structured products has coincided with a proliferation of fixed income models used to analyze hedging, pricing, forecasting, and estimation for the term structure of interest rates. This article evaluates five models Ho-Lee (HL); Black-Derman-Toy (BDT); Vasicek; Cox-Ingersoll-Ross (CIR); and Heath-Jarrow-Morton (HJM) (see Exhibit 1) that are currently used by structured finance practitioners. We suggest which models are most appropriate for assets with different time horizons, interest rate sensitivities and cashflow properties. The authors link model selection to structured financial instruments with the singular focus on the trade-off between model precision/complexity and calculation costs. {\textcopyright} Emerald Backfiles 2007.},
author = {{Paul Joshi}, J and Swertloff, Larry},
doi = {10.1108/eb022940},
issn = {09657967},
journal = {Journal of Risk Finance},
number = {1},
pages = {106--114},
title = {{A user's guide to interest rate models: Applications for structured finance}},
volume = {1},
year = {1999}
}
@book{Nelder1989,
author = {McCullagh, Peter and Nelder, John Ashworth},
publisher = {Chapman \& Hall},
title = {{Generalized Linear Models, Second Edition}},
year = {1989}
}
@article{Venter2003,
abstract = {Stochastic models for interest rates are reviewed and fitting methods are discussed. Tests for the dynamics of short-term rates are based on model fits. A method of testing yield curve distribu- tions for use in insurer asset scenario generators is introduced. This uses historical relationships in the conditional distributions of yield spreads given the short-term rate. As an illustration, this method is used to test a few selected models.},
author = {Venter, Gary G},
journal = {CAS Forum},
pages = {647--674},
title = {{Testing Stochastic Interest Rate Generators for Insurer Risk and Capital Models}},
url = {http://www.casact.org/pubs/forum/03wforum/03wf647c.pdf},
year = {2003}
}
@inproceedings{FIoA1993_,
author = {{Faculty and Institute of Actuaries}},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Faculty and Institute of Actuaries - 1993 - Prudential margins 1993 General Insurance Convention.pdf:pdf},
title = {{Prudential margins: 1993 General Insurance Convention}},
year = {1993}
}
@article{Currie2016,
abstract = {Many common models of mortality can be expressed compactly in the language of either generalized linear models or generalized non-linear models. The R language provides a description of these models which parallels the usual algebraic definitions but has the advantage of a transparent and flexible model specification. We compare eight model structures for mortality. For each structure, we consider (a) the Poisson models for the force of mortality with both log and logit link functions and (b) the binomial models for the rate of mortality with logit and complementary log–log link functions. Part of this work shows how to extend the usual smooth two-dimensional P-spline model for the force of mortality with Poisson error and log link to the other smooth two-dimensional P-spline models with Poisson and binomial errors defined in (a) and (b). Our comments are based on the results of fitting these models to data from six countries: Australia, France, Japan, Sweden, UK and USA. We also discuss the possibility of forecasting with these models; in particular, the introduction of cohort terms generally leads to an improvement in overall fit, but can also make forecasting with these models problematic.},
author = {Currie, Iain D.},
doi = {10.1080/03461238.2014.928230},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Currie - 2016 - On fitting generalized linear and non-linear models of mortality.pdf:pdf},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {R language,constraints,forecasting,generalized linear models,identifiability,mortality},
number = {4},
pages = {356--383},
title = {{On fitting generalized linear and non-linear models of mortality}},
volume = {2016},
year = {2016}
}
@article{Pricing2014,
author = {Pricing, Option and Short, When and Are, Rates and Karasinski, Piotr},
number = {4},
pages = {52--59},
title = {{Option Pricing when Short Rates are Lognormal}},
volume = {47},
year = {2014}
}
@article{Cairns2011,
abstract = {In the first part of the paper, we consider the wide range of extrapolative stochastic mortality models that have been proposed over the last 15-20 years. A number of models that we consider are framed in discrete time and place emphasis on the statistical aspects of modelling and forecasting. We discuss how these models can be evaluated, compared and contrasted. We also discuss a discrete-time market model that facilitates valuation of mortality-linked contracts with embedded options. We then review several approaches to modelling mortality in continuous time. These models tend to be simpler in nature, but make it possible to examine the potential for dynamic hedging of mortality risk. Finally, we review a range of financial instruments (traded and over-the-counter) that could be used to hedge mortality risk. Some of these, such as mortality swaps, already exist, while others anticipate future developments in the market.},
author = {Cairns, Andrew J. G. and Blake, David P. and Dowd, Kevin},
doi = {10.2139/ssrn.1339970},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Cairns, Blake, Dowd - 2011 - Modelling and Management of Mortality Risk A Review.pdf:pdf},
journal = {SSRN Electronic Journal},
number = {April 2018},
title = {{Modelling and Management of Mortality Risk: A Review}},
year = {2011}
}
@article{Cairns2014,
author = {Cairns, Andrew},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Cairns - 2014 - Interest-Rate Models.pdf:pdf},
number = {July 2003},
title = {{Interest-Rate Models}},
year = {2014}
}
@misc{Kagan2020,
author = {Kagan, Julia},
month = {aug},
title = {{Solvency Capital Requirement (SCR)}},
url = {https://www.investopedia.com/terms/s/solvency-capital-requirement.asp},
urldate = {2021-03-13},
year = {2020}
}
@article{Lai1995,
author = {Lai, Siu-Wai and Frees, Edward W},
doi = {10.2307/253822},
issn = {00224367},
journal = {The Journal of Risk and Insurance},
number = {3},
pages = {535},
title = {{Examining Changes in Reserves Using Stochastic Interest Models}},
volume = {62},
year = {1995}
}
@article{HO1986,
abstract = {This paper derives an arbitrage‐free interest rate movements model (AR model). This model takes the complete term structure as given and derives the subsequent stochastic movement of the term structure such that the movement is arbitrage free. We then show that the AR model can be used to price interest rate contingent claims relative to the observed complete term structure of interest rates. This paper also studies the behavior and the economics of the model. Our approach can be used to price a broad range of interest rate contingent claims, including bond options and callable bonds. 1986 The American Finance Association},
author = {HO, THOMAS S Y and LEE, SANG‐BIN ‐B},
doi = {10.1111/j.1540-6261.1986.tb02528.x},
issn = {15406261},
journal = {The Journal of Finance},
number = {5},
pages = {1011--1029},
title = {{Term Structure Movements and Pricing Interest Rate Contingent Claims}},
volume = {41},
year = {1986}
}
@article{Vasicek1977,
abstract = {The paper derives a general form of the term structure of interest rates. The following assumptions are made: (A.1) The instantaneous (spot) interest rate follows a diffusion process; (A.2) the price of a discount bond depends only on the spot rate over its term; and (A.3) the market is efficient. Under these assumptions, it is shown by means of an arbitrage argument that the expected rate of return on any bond in excess of the spot rate is proportional to its standard deviation. This property is then used to derive a partial differential equation for bond prices. The solution to that equation is given in the form of a stochastic integral representation. An interpretation of the bond pricing formula is provided. The model is illustrated on a specific case. {\textcopyright} 1977.},
author = {Vasicek, Oldrich},
doi = {10.1016/0304-405X(77)90016-2},
issn = {0304405X},
journal = {Journal of Financial Economics},
number = {2},
pages = {177--188},
title = {{An equilibrium characterization of the term structure}},
volume = {5},
year = {1977}
}
@article{Amini2012,
author = {Amini, Hamed},
journal = {MSc Thesis,},
number = {Delft University},
title = {{Calibration of different interest rate models for a good fit of yield curves}},
url = {http://repository.tudelft.nl/assets/uuid:5ae7b593-8c79-4947-8581-c9e13a6f2986/Final_Thesis.pdf},
year = {2012}
}
@article{Renard2013,
abstract = {In broad sense, modelling refers to the process of generating a simplified representation of a real system. A suitable model must be able to explain past observations, integrate present data and predict with reasonable accuracy the response of the system to planned stresses (Carrera et al., 1987). Models have evolved together with science and nowadays modelling is an essential and inseparable part of scientific activity. In environmental sciences, models are used to guarantee suitable conditions for sustainable development and are a pillar for the design of social and industrial policies. Model types include analogue models, scale models and mathematical models. Analogue models represent the target system by another, more understandable or analysable system. These models rely on Feynman's principle (Feynman et al., 1989, sec. 12-1): 'The same equations have the same solutions.' For example, the electric/hydraulic analogy (Figure 8.1a) establishes the parallelism between voltage and water-pressure difference or between electric current and flow rate of water. Scale models are representations of a system that is larger or smaller (most often) than the actual size of the system being modelled. Scale models (Figure 8.1b) are often built to analyse physical processes in the laboratory or to test the likely performance of a particular design at an early stage of development without incurring the full expense of a full-sized prototype. Notwithstanding the use of these types of models in other branches of science and engineering , the most popular models in environmental sciences are mathematical. A mathematical model describes a system by a set of state variables and a set of equations that establish relationships between those variables and the governing parameters. Mathematical models can be analytical or numerical. Analytical models often require many simplifications to render the equations amenable to solution. Instead, numerical models are more versatile and make use of computers to solve the equations. Mathematical models (either analytical or numerical) can be deterministic or stochastic (from the Greek $\tau$ ´ o$\chi$oς for 'aim' or 'guess'). A deterministic model is one in which state variables are uniquely determined by parameters in the model and by sets of previous states of these variables. Therefore, deterministic models perform the same way for a given set of parameters and initial conditions and their solution is unique. Nevertheless, deterministic models are sometimes unstable-i.e., small perturbations (often below the detection limits) of the initial conditions or the parameters governing the problem lead to large variations of the final solution (Lorenz, 1963). Thus, despite the fact that the solution is unique, one can obtain solutions that are dramatically different by perturbing slightly a single governing parameter or the initial condition at a single point of the domain.},
author = {Renard, Philippe and Alcolea, Andres and Ginsbourger, David},
file = {::},
journal = {Environmental Modelling:Finding Simplicity in Complexity},
pages = {133--149},
title = {{Stochastic versus Deterministic Approaches}},
volume = {2},
year = {2013}
}
@article{Hull1990,
author = {Hull, John},
number = {4},
pages = {573--592},
title = {{Pricing Interest-Rate- Derivative Securities}},
volume = {3},
year = {1990}
}
@article{Ornstein2016,
author = {Ornstein, The and Ornstein, Leonard and Uhlenbeck, George Eugene},
pages = {1--11},
title = {{Calibration of the Vasicek Model: An Step by Step Guide}},
year = {2016}
}
@misc{SARB2000,
author = {{South African Reserve Bank}},
title = {{Monetary Policy}},
url = {https://www.resbank.co.za/en/home/what-we-do/monetary-policy},
urldate = {2021-08-23},
year = {2000}
}
@article{Cairns2011b,
abstract = {This paper introduces a new framework for modelling the joint development over time of mortality rates in a pair of related populations with the primary aim of producing consistent mortality forecasts for the two populations. The primary aim is achieved by combining a number of recent and novel developments in stochastic mortality modelling, but these, additionally, provide us with a number of side benefi ts and insights for stochastic mortality modelling. By way of example, we propose an Age-Period-Cohort model which incorporates a mean-reverting stochastic spread that allows for different trends in mortality improvement rates in the short-run, but parallel improvements in the long run. Second, we fi t the model using a Bayesian framework that allows us to combine estimation of the unobservable state variables and the parameters of the stochastic processes driving them into a single procedure. Key benefi ts of this include dampening down of the impact of Poisson variation in death counts, full allowance for parameter uncertainty, and the fl exibility to deal with missing data. The framework is designed for large populations coupled with a small sub-population and is applied to the England  &  Wales national and Continuous Mortality Investigation assured lives males populations. We compare and contrast results based on the twopopulation approach with single-population results. {\textcopyright} 2011 by Astin Bulletin. All rights reserved.},
author = {Cairns, Andrew J.G. and Blake, David and Dowd, Kevin and Coughlan, Guy D. and Khalaf-Allah, Marwa},
doi = {10.2143/AST.41.1.2084385},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Cairns et al. - 2011 - Bayesian stochastic mortality modeling for two populations.pdf:pdf},
issn = {1783-1350},
journal = {Astin Bulletin},
number = {01},
title = {{Bayesian stochastic mortality modeling for two populations}},
volume = {41},
year = {2011}
}
@article{Feyter2006,
abstract = {Threshold selection is a key aspect in extreme values analysis, especially\nwhen the sample size is small. The main idea underpinning this work\nis that extreme observations are assumed to be outliers of a specified\nparametric model. We propose a threshold selection method based on\noutlier detection using a suitable measure of surprise. Copyright\n(c) 2006 John Wiley & Sons, Ltd.},
author = {Feyter, Tim De},
doi = {10.1002/asmb},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Feyter - 2006 - Modelling heterogeneity in manpower planning dividing the personnel system into more homogeneous subgroups.pdf:pdf},
journal = {Applied Stochastic Models in Business and Industry},
keywords = {homogeneity,manpower planning,markov models,stochastic models},
number = {March},
pages = {321--334},
title = {{Modelling heterogeneity in manpower planning : dividing the personnel system into more homogeneous subgroups}},
year = {2006}
}
@article{Merton1973,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor. The long history of the theory of option pricing began in 1900 when the French mathematician Louis Bachelier deduced an option pricing formula based on the assumption that stock prices follow a Brownian motion with zero drift. Since that time, numerous researchers have contributed to the theory. The present paper begins by deducing a set of r estrictions on option pricing formulas from the assumption that in-vestors prefer more to less. These restrictions are necessary conditions for a formula to be consistent with a rational pricing theory. Attention is given to the problems created when dividends are paid on the under-lying common stock and when the terms of the option contract can be changed explicitly by a change in exercise price or implicitly by a shift in the investment or capital structure policy of the firm. Since the de-duced restrictions are not sufficient to uniquely determine an option pricing formula, additional assumptions are introduced to examine and extend the seminal Black-Scholes theory of option pricing. Explicit formulas for pricing both call and put options as well as.for warrants and the new "down-and-out" option are derived. The effects of dividends and call provisions on the warrant price are examined. The possibilities for further extension of the theory to the pricing of corporate liabilities are discussed.},
author = {Merton, Robert C},
journal = {Source: The Bell Journal of Economics and Management Science},
number = {1},
pages = {141--183},
title = {{The RAND Corporation Theory of rational option pricing}},
url = {http://www.jstor.org/stable/3003143%0Ahttp://www.jstor.org/page/info/about/policies/terms.jsp%0Ahttp://www.jstor.org},
volume = {4},
year = {1973}
}
@article{Plat2009,
abstract = {In the last decennium a vast literature on stochastic mortality models has been developed. All well-known models have nice features but also disadvantages. In this paper a stochastic mortality model is proposed that aims at combining the nice features from the existing models, while eliminating the disadvantages. More specifically, the model fits historical data very well, is applicable to a full age range, captures the cohort effect, has a non-trivial (but not too complex) correlation structure and has no robustness problems, while the structure of the model remains relatively simple. Also, the paper describes how to incorporate parameter uncertainty in the model. Furthermore, a risk neutral version of the model is given, that can be used for pricing. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Plat, Richard},
doi = {10.1016/j.insmatheco.2009.08.006},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Plat - 2009 - On stochastic mortality modeling.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
number = {3},
title = {{On stochastic mortality modeling}},
volume = {45},
year = {2009}
}
@article{Brigo2001,
abstract = {"It will be short: the znterim is mine. And a man's life is no more than to say lone' ,) Hamlet, V.2 "Pilder on!" rPiloier Jon! Koji Kabuto, assu111ing control of Mazinger Z (1972) 3.1 Introduction and Guided Tour The theory of interest-rate modeling was originally based on the assunlption of specific one-dinlensional dynamics for the instantaneous spot rate process r. $\sim$ 10deling directly such dynanlics is very convenient since all fundam utal quantities (rates and bonds) are readily defined, by no-arbitrage arguments as the expectation of a functional of the process 7'. Indeed, the existence of a risk-neutral Ineasure implies that the arbitrage-free prlce at time t of a contingent claim with payoff HT at time T is given by H t = E t D(t, T) H T  = E t e-ItT r(s)ds Hr 1 (3.1) with E t denoting the time t-conditional expectation under that measure. In particular, the zero-coupon-bond price at tinle t for the nlaturity T is characterized by a unit amount of currency available at time T, so that HT = 1 and we obtain (3.2) From this last expression it is clear that whenever we can characterize the distribution of e-ItT r(s)ds in ternlS of a chosen dynanlics for T 1 conditional on the information available at time t, we a.re able to C0111pute bond pric s P. As we ha.ve seen earlier in Chapter 1, from bond prices all kind of rates are available, so that indeed the whole zero-coupon curve is characterized in terms of distributional properties of T. The pioneering approach proposed by Vasicek (1977) was based on defining the instantaneous-spot-rate dynamics under the real-world measure. His derivation of an arbitrage-free price for any interest-rate derivative followed from using the basic Black and Scholes (1973) argunlents, while taking into account the non-tradable feature of interest rates.},
author = {Brigo, Damiano and Mercurio, Fabio},
doi = {10.1007/978-3-662-04553-4_3},
pages = {43--125},
title = {{One-factor short-rate models}},
volume = {1},
year = {2001}
}
@article{Brouhns2005,
abstract = {This paper proposes bootstrap procedures for expected remaining lifetimes and life annuity single premiums in a dynamic mortality environment. Assuming a further continuation of the stable pace of mortality decline, a Poisson log-bilinear projection model is applied to the forecasting of the gender- and age-specific mortality rates for Belgium on the basis of mortality statistics relating to the period 1950–2000. Bootstrap procedures are then used to obtain confidence intervals on the aforementioned quantities. {\textcopyright} 2005 Taylor & Francis Group, LLC.},
author = {Brouhns, Natacha and Denuit, Michel and {Van Keilegom}, Ingrid},
doi = {10.1080/03461230510009754},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Brouhns, Denuit, Van Keilegom - 2005 - Bootstrapping the poisson log-bilinear model for mortality forecasting.pdf:pdf},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {Age-sex-specific mortality,Bootstrap,Confidence intervals,Mortality forecasting,Poisson regression,Projected lifetables},
number = {3},
pages = {212--224},
title = {{Bootstrapping the poisson log-bilinear model for mortality forecasting}},
volume = {2005},
year = {2005}
}
@article{Leippold2005,
abstract = {In this article we implement the well known Ho-Lee Model of the term structure of interest rates and describe the algorithm behind this model. After a brief discussion of interest rates and bonds we construct a binomial tree and show how to replicate any fixed income type security. This allows us to value any interest rate contingent claim by means of the replicating portfolio. We also discuss the problem of negative interest rates arising in this model and show how to calibrate the model to an observed set of bond prices.},
author = {Leippold, Markus and Wiener, Zvi},
doi = {10.2139/ssrn.292225},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
pages = {1--20},
title = {{Algorithms behind Term Structure Models of Interest Rates: I. Valuation and Hedging of Interest Rates Derivatives with the Ho-Lee Model}},
year = {2005}
}
@article{Blanchet2019,
abstract = {In this paper, we study the pricing of life insurance portfolios in the presence of dependent lives. We assume that an insurer with an initial exposure to n mortality-contingent contracts wanted to acquire a second portfolio constituted of m contracts. The policyholders' lifetimes in these portfolios are correlated with a Farlie-Gumbel-Morgenstern (FGM) copula, which induces a dependency between the two portfolios. In this setting, we compute the indifference price charged by the insurer endowed with an exponential utility. The indifference price is characterized as a solution to a backward stochastic differential equation (BSDE), which can be decomposed into (n − 1) n! auxiliary BSDEs. In this general case, the derivation of the indifference price is computationally infeasible. Therefore, while focusing on the example of death benefit contracts, we develop a model-point based approach in order to ease the computation of the price. It consists on replacing each portfolio with a single policyholder that replicates some risk metrics of interest. Also, the two representative contracts should adequately reproduce the observed dependency between the initial portfolios. We implement the proposed procedure and compare the computed prices to classical valuation approach.},
author = {Blanchet-Scalliet, Christophette and Dorobantu, Diana and Salhi, Yahia},
doi = {10.1007/S11009-017-9611-2},
file = {::},
journal = {Methodology and Computing in Applied Probability},
keywords = {Indifference pricing,Life insurance,Representative contract,Utility maximization},
month = {jun},
number = {2},
pages = {423--448},
publisher = {Springer New York LLC},
title = {{A Model-Point Approach to Indifference Pricing of Life Insurance Portfolios with Dependent Lives}},
volume = {21},
year = {2019}
}
@book{Gaebler2014a,
author = {Gaebler, Ralph F.},
booktitle = {Sources of State Practice in International Law: Second Revised Edition},
doi = {10.1163/9789004272224_029},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Gaebler - 2014 - Republic of South Africa.pdf:pdf},
isbn = {9789004272224},
number = {39403},
pages = {468--481},
title = {{Republic of South Africa}},
year = {2014}
}
@article{Cairns2006a,
abstract = {It is now widely accepted that stochastic mortality – the risk that aggregate mortality might differ from that anticipated – is an important risk factor in both life insurance and pensions. As such it affects how fair values, premium rates, and risk reserves are calculated.This paper makes use of the similarities between the force of mortality and interest rates to examine how we might model mortality risks and price mortality-related instruments using adaptations of the arbitrage-free pricing frameworks that have been developed for interest-rate derivatives. In so doing, the paper pulls together a range of arbitrage-free (or risk-neutral) frameworks for pricing and hedging mortality risk that allow for both interest and mortality factors to be stochastic. The different frameworks that we describe – short-rate models, forward-mortality models, positive-mortality models and mortality market models – are all based on positive-interest-rate modelling frameworks since the force of mortality can be treated in a similar way to the short-term risk-free rate of interest. While much of this paper is a review of the possible frameworks, the key new development is the introduction of mortality market models equivalent to the LIBOR and swap market models in the interest-rate literature.These frameworks can be applied to a great variety of mortality-related instruments, from vanilla longevity bonds to exotic mortality derivatives.},
author = {Cairns, Andrew J.G. and Blake, David and Dowd, Kevin},
doi = {10.2143/ast.36.1.2014145},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Cairns, Blake, Dowd - 2006 - Pricing Death Frameworks for the Valuation and Securitization of Mortality Risk.pdf:pdf},
issn = {0515-0361},
journal = {ASTIN Bulletin},
number = {01},
pages = {79--120},
title = {{Pricing Death: Frameworks for the Valuation and Securitization of Mortality Risk}},
volume = {36},
year = {2006}
}
@article{Tilley2002,
author = {Tilley, Peter D and Szatzschneider, Wojciech},
number = {1},
pages = {1--26},
title = {{Cox, Ingersoll and Ross Models of Interest Rates}},
volume = {28},
year = {2002}
}
@article{Cairns2009,
abstract = {We compare quantitatively eight stochastic models explaining improvements in mortality rates in England and Wales and in the United States. On the basis of the Bayes Information Criterion (BIC), we find that, for higher ages, an extension of the Cairns-Blake-Dowd (CBD) model that incorporates a cohort effect fits the England and Wales males data best, while for U.S. males data, the Renshaw and Haberman (RH) extension to the Lee and Carter model that also allows for a cohort effect provides the best fit. However, we identify problems with the robustness of parameter estimates under the RH model, calling into question its suitability for forecasting. A different extension to the CBD model that allows not only for a cohort effect, but also for a quadratic age effect, while ranking below the other models in terms of the BIC, exhibits parameter stability across different time periods for both datasets. This model also shows, for both datasets, that there have been approximately linear improvements over time in mortality rates at all ages, but that the improvements have been greater at lower ages than at higher ages, and that there are significant cohort effects. {\textcopyright} 2009 Taylor & Francis Group, LLC.},
author = {Cairns, Andrew J.G. and Blake, David and Dowd, Kevin and Coughlan, Guy D. and Epstein, David and Ong, Alen and Balevich, Igor},
doi = {10.1080/10920277.2009.10597538},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Cairns et al. - 2009 - A Quantitative Comparison of Stochastic Mortality Models Using Data From England and Wales and the United States.pdf:pdf},
issn = {10920277},
journal = {North American Actuarial Journal},
number = {1},
pages = {1--35},
title = {{A Quantitative Comparison of Stochastic Mortality Models Using Data From England and Wales and the United States}},
volume = {13},
year = {2009}
}
@article{Oliveira2014,
abstract = {The efficiency of traditional and stochastic interest rate risk measures is compared under one-, two-, and three-factor no-arbitrage Gauss-Markov term structure models, and for different immunization periods. The empirical analysis, run on the German Treasury bond market from January 2000 to December 2010, suggests that: i) Stochastic interest rate risk measures provide better portfolio immunization than the Fisher-Weil duration; and ii) The superiority of the stochastic risk measures is more evident for multi-factor models and for longer investment horizons. These findings are supported by a first-order stochastic dominance analysis, and are robust against yield curve estimation errors.},
author = {Oliveira, Lu{\'{i}}s and {Vidal Nunes}, Jo{\~{a}}o Pedro and Malcato, Lu{\'{i}}s},
doi = {10.1007/s10258-014-0104-8},
issn = {16179838},
journal = {Portuguese Economic Journal},
keywords = {Asset-liability management,Immunization strategies,Interest rate risk,Stochastic dominance,Stochastic duration},
number = {3},
pages = {141--165},
title = {{The performance of deterministic and stochastic interest rate risk measures: Another Question of Dimensions?}},
volume = {13},
year = {2014}
}
@article{Aas2018,
abstract = {Under the Solvency II regulatory framework it is essential for life insurers to have an adequate interest rate model. In this paper, we investigate whether the choice of the interest rate model has an impact on the valuation of the best estimate of the liabilities. We use three well-known interest rate models; the CIR++-model, the G2++-model and the Libor Market model. Our numerical results show that for low to medium durations of the liabilities and a relatively low proportion of credit bonds in the asset portfolio, the three interest rate models produce quite similar values for the best estimate liabilities. However, for large durations of the liabilities, or a large bond proportion, or both, the differences can be quite large. There is no easy answer to the question of which model should be used in cases where the choice of interest rate model has a significant impact. Based on the study described in this paper, our advice is to use the G2++-model, which seems to represent an appropriate trade-off between accuracy and complexity.},
author = {Aas, Kjersti and Neef, Linda R and Williams, Lloyd and Raabe, Dag},
doi = {10.1080/03461238.2017.1332679},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {CIR++,G2++,Interest rate models,Libor Market model,life insurance,market value of liabilities},
number = {3},
pages = {203--224},
title = {{Interest rate model comparisons for participating products under Solvency II}},
volume = {2018},
year = {2018}
}
@article{Zellner1963,
abstract = {The finite sample properties of an asymptotically efficient technique (JASA, June, 1962) for estimating coefficients in certain generally encountered sets of regression equations are studied in this paper. In particular, exact first and second moments of the asymptotically efficient coefficient estimator are derived and compared with those of the usual least squares estimator. Further, the exact probability density function of the new estimator is derived and studied as a function of sample size. It is found that the approach to asymptotic normality is fairly rapid and that for a wide range of conditions an appreciable part of the asymptotic gain in efficiency is realized in samples of finite size. {\textcopyright} Taylor & Francis Group, LLC.},
author = {Zellner, Arnold},
doi = {10.1080/01621459.1963.10480681},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Zellner - 1963 - Estimators for Seemingly Unrelated Regression Equations Some Exact Finite Sample Results.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {304},
pages = {977--992},
title = {{Estimators for Seemingly Unrelated Regression Equations: Some Exact Finite Sample Results}},
volume = {58},
year = {1963}
}
@book{Marocco1998,
address = {Birmingham},
author = {Marocco, P and Pitacco, E},
pages = {453--479},
title = {{Longevity risk and life annuity reinsurance}},
volume = {Vol 6},
year = {1998}
}
@misc{Risk.net,
author = {Risk.net},
title = {{Solvency capital requirement (SCR) definition - Risk.net}},
url = {https://www.risk.net/definition/solvency-capital-requirement-scr},
urldate = {2021-03-13},
year = {2021}
}
@article{Renshaw2008,
abstract = {This paper provides a comparative study of simulation strategies for assessing risk in mortality rate predictions and associated estimates of life expectancy and annuity values in both period and cohort frameworks. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
annote = {We do not, however, subscribe to the assertion (Li et al.
(2006, p. 14, Section 6)), requiring internal consistency in the relationship between the probability function of the responses Dxt and their first two moments E(Dxt ) and Var(Dxt ) as a general principle of modern statistical modelling: Section 5 being a case in point. We quote from the outer cover of McCullagh and Nelder (1989) ‘An important feature [of generalised linear and non-linear modelling] is that the principal conclusions depend only on [first and] second moment
814 A.E. Renshaw, S. Haberman / Insurance: Mathematics and Economics 42 (2008) 797–816
Theoretical Simulated
Estimated drift parameter 95% confidence interval Predicted (2013) time series 95% prediction interval $\theta$˜ = −1.58 (−2.60, −0.61) $\theta$ˆ = −1.57 (−2.60, −0.54)
$\kappa$˙2013 = −15.74 (−30.30, −1.18) $\kappa$˙2013 = −15.79 (−25.95, −6.12)
Box III.
assumptions as opposed to the complete correctness of an assumed probability function'.},
author = {Renshaw, A. E. and Haberman, S.},
doi = {10.1016/j.insmatheco.2007.08.009},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Renshaw, Haberman - 2008 - On simulation-based approaches to risk measurement in mortality with specific reference to Poisson Lee-Carter.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Joint modelling,Mortality projections,Mortality statistics,Negative binomial modelling,Over-dispersion,Poisson modelling,Simulated risk},
number = {2},
pages = {797--816},
title = {{On simulation-based approaches to risk measurement in mortality with specific reference to Poisson Lee-Carter modelling}},
volume = {42},
year = {2008}
}
@article{Tilley,
author = {Tilley, James A},
pages = {287--316},
title = {{VII An Actuarial Layman ' s Guide Interest Rate Generators}}
}
@article{Chatterjee2005,
abstract = {The purpose of this paper is to see how the term structure of interest rates has evolved the sterling and euro treasury bond markets over the period 1999-2003. German have been used as a proxy for euro-denominated bonds. A state-space for the single-factor Cox, Ingersoll and Ross (1985) model is employed analyse the intertemporal dynamics of the term structure. Quasi-maximum estimates of the model parameters are obtained by using the Kalman filter calculate the likelihood function. Results of the empirical analysis show that while unobserved instantaneous interest rate exhibits mean reverting behaviour in both UK and Germany, the mean reversion of the interest rate process has been slower in the UK. The volatility component, which shocks the process at step in time is also higher in the UK as compared to Germany.},
author = {Chatterjee, Somnath},
keywords = {Kalman filter,panel data,term Structure},
pages = {1--33},
title = {{Application of the Kalman filter for estimating continuous time term structure models: the case of UK and Germany}},
url = {http://hdl.handle.net/1905/355},
year = {2005}
}
@article{Turner1993,
author = {Turner, B Y Mark},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Turner - 1993 - Jss 33 (1993) 1-36 phi reserving.pdf:pdf},
number = {December 1988},
pages = {1--36},
title = {{Jss 33 (1993) 1-36 phi reserving}},
volume = {33},
year = {1993}
}
@article{Renshaw2006,
abstract = {The Lee-Carter modelling framework is extended through the introduction of a wider class of generalised, parametric, non-linear models. This permits the modelling and extrapolation of age-specific cohort effects as well as the more familiar age-specific period effects. The choice of error distribution is generalised. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Renshaw, A. E. and Haberman, S.},
doi = {10.1016/j.insmatheco.2005.12.001},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Renshaw, Haberman - 2006 - A cohort-based extension to the Lee-Carter model for mortality reduction factors.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Cohort effects,Generalised non-linear models,Mortality projections,Mortality reduction factors,Time series},
number = {3},
pages = {556--570},
title = {{A cohort-based extension to the Lee-Carter model for mortality reduction factors}},
volume = {38},
year = {2006}
}
@article{Villegas2018,
abstract = {In this paper we mirror the framework of generalized (non-)linear models to define the family of generalized age-period-cohort stochastic mortality models which encompasses the vast majority of stochastic mortality projection models proposed to date, including the well-known Lee-Carter and Cairns-Blake-Dowd models. We also introduce the R package StMoMo which exploits the unifying framework of the generalized age-period-cohort family to provide tools for fitting stochastic mortality models, assessing their goodness-of-fit and performing mortality projections. We illustrate some of the capabilities of the package by performing a comparison of several stochastic mortality models applied to the England and Wales population.},
author = {Villegas, Andr{\'{e}}s M. and Millossovich, Pietro and Kaishev, Vladimir K.},
doi = {10.18637/jss.v084.i03},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Villegas, Millossovich, Kaishev - 2018 - StMoMo Stochastic mortality modeling in R.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Age-period-cohort,Generalized nonlinear models,Mortality forecasting,Mortality modeling},
number = {3},
title = {{StMoMo: Stochastic mortality modeling in R}},
volume = {84},
year = {2018}
}
@misc{CorporateFinanceInstitute,
author = {{Corporate Finance Institute}},
title = {{Cash Reserves - Definition, Benefits and How to Calculate}},
url = {https://corporatefinanceinstitute.com/resources/knowledge/finance/cash-reserves/},
urldate = {2021-03-26},
year = {2021}
}
@article{Haberman2011,
abstract = {The relative merits of different parametric models for making life expectancy and annuity value predictions at both pensioner and adult ages are investigated. This study builds on current published research and considers recent model enhancements and the extent to which these enhancements address the deficiencies that have been identified of some of the models. The England & Wales male mortality experience is used to conduct detailed comparisons at pensioner ages, having first established a common basis for comparison across all models. The model comparison is then extended to include the England & Wales female experience and both the male and female USA mortality experiences over a wider age range, encompassing also the working ages. {\textcopyright} 2010 Elsevier B.V.},
author = {Haberman, Steven and Renshaw, Arthur},
doi = {10.1016/j.insmatheco.2010.09.003},
file = {:C\:/Users/User-PC/Documents/Mendeley Desktop/Haberman, Renshaw - 2011 - A comparative study of parametric mortality projection models.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Age-period effects,Age-period-cohort effects,Back-fitting,Binomial response models,Forecast statistics,Model and forecast comparison,Mortality forecasting},
number = {1},
pages = {35--55},
publisher = {Elsevier B.V.},
title = {{A comparative study of parametric mortality projection models}},
url = {http://dx.doi.org/10.1016/j.insmatheco.2010.09.003},
volume = {48},
year = {2011}
}
@article{Wijck2006,
author = {Wijck, Van},
journal = {Management},
number = {February},
title = {{Interest rate model theory with reference to the South African market /}},
year = {2006}
}
